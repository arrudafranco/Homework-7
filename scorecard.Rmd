---
title: "Predicting student debt load"
author: "Gustavo Arruda"
date: "`r lubridate::today()`"
output: github_document
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

## Load necessary libraries

```{r}
library(tidyverse)
library(rcfss)
library(leaps)
library(caret)
library(ggplot2)
library(knitr)
library(broom)
set.seed(1234)
```

## Single Variable Model

```{r}

ggplot(scorecard, aes(cost, debt)) +
  geom_point() +
  geom_smooth(method = "lm")

```

## Multiple Variables Model

```{r}
scorecard_num <- select(scorecard, admrate:debt)
#Limiting the model to numerical variables.

# Source of help:
#http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/154-stepwise-regression-essentials-in-r/

# Set up repeated k-fold cross-validation
train_control <- trainControl(method = "cv", number = 10)

# Train the model
back_selection <- train(debt ~., data = scorecard_num,
                    method = "leapBackward", 
                    tuneGrid = data.frame(nvmax = 1:7),
                    trControl = train_control,
                    na.action = na.omit
                    )

ggplot(back_selection) +
  labs(y = "Root Mean Squared Error (Cross-Validation)")

summary(back_selection$finalModel)

scorecard_model <- lm(debt ~ admrate + satavg + cost + comprate + firstgen, data = scorecard) %>%
  tidy()
kable(scorecard_model)

```
I used a backward variable selection algorithm to decide which variables to use in my multivariate linear model. In those kinds of algorithms we start using all variables, then iteratively test models by removing each variable at a time until the change is statistically significant. The issue with variable selection algorithms is that models might get overfitted to the training data set and perform significantly worse with subsequent testing data. To avoid overfitting I used a k-fold cross-validation algorithm. That method iteratively trains models with a random fraction of the data then tests them with the remaining data.

The RMSE ~ Maximum Number of Predictors chart shows that the models with the least error given my parameters were the ones with 5 predictor variables. Under further examination, the model indicates that the best predictor variables are admrate, satavg, cost, comprate and firstgen. I included a table calculating their coefficients and standard error.

The best predicting variable does seem to be cost followed by satavg, with relatively much lower standard errors. The costlier the cost of attendance, the highest the median student debt, which is somewhat self-explanatory. The average SAT admission score coefficient is more puzzling to me. The highest the SAT score, the lowest the student debt. My hunch here is that there is a confounding variable: the average social standing of the student body directly influences their ability to prepare for the SATs. Hence, a higher average SAT score would mean less need to take on debt. Following those in the order of increasing standard error, admissions rates have a comparatively higher positive coefficient with student debt. In this case, for profit colleges come to mind: the easiest it is to get into a college, the larger the probability of that given college being for profit which would lead to higher student debt. Subsequentially, completion rates have a positive coefficient and proportion of first generation student body have a negative coefficient with student debt, although they both have much higher standard error. Again, lower proportion of first generation students might have the confounding variable of average social standing of the student body with median student debt.

## Session info

```{r}
devtools::session_info()
```
